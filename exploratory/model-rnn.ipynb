{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN - Sentimen Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib\n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install sklearn\n",
    "!pip install keras\n",
    "!pip install tensorflow\n",
    "!pip install seaborn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alurnya kaya gini:\n",
    "# 1. bikin sequence dari teks yang udah dikasi pos\n",
    "# 2. bikin pad sequence\n",
    "# 3. bikin rnn\n",
    "# 4. selesai wkwk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_text</th>\n",
       "      <th>1st_clean_twitter</th>\n",
       "      <th>1st_clean_twitter_gensim</th>\n",
       "      <th>2nd_punctuation</th>\n",
       "      <th>2nd_punctuation_gensim</th>\n",
       "      <th>3rd_emoji</th>\n",
       "      <th>3rd_emoji_gensim</th>\n",
       "      <th>4th_tokenized</th>\n",
       "      <th>4th_tokenized_gensim</th>\n",
       "      <th>sentence</th>\n",
       "      <th>5_tagged</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cepet bgt si‚Ä¶. #10YearsOf5SOS</td>\n",
       "      <td>cepet bgt si‚Ä¶.  10YearsOf5SOS</td>\n",
       "      <td>cepet bgt si‚Ä¶.</td>\n",
       "      <td>cepet bgt si‚Ä¶  10YearsOf5SOS</td>\n",
       "      <td>cepet bgt si‚Ä¶.</td>\n",
       "      <td>cepet bgt si‚Ä¶  10YearsOf5SOS</td>\n",
       "      <td>cepet bgt si‚Ä¶.</td>\n",
       "      <td>[cepet, bgt, si‚Ä¶, 10YearsOf5SOS]</td>\n",
       "      <td>[cepet, bgt, si‚Ä¶, .]</td>\n",
       "      <td>Sentence: \"cepet bgt si‚Ä¶ 10YearsOf5SOS\"   [‚àí T...</td>\n",
       "      <td>[cepet_PROPN, bgt_PROPN, si‚Ä¶_PROPN, 10YearsOf5...</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#10YearsOf5SOS udah 10 tahun ajaa time flies.....</td>\n",
       "      <td>10YearsOf5SOS udah 10 tahun ajaa time flies.....</td>\n",
       "      <td>udah 10 tahun ajaa time flies.. eh tapi aku b...</td>\n",
       "      <td>10YearsOf5SOS udah 10 tahun ajaa time flies e...</td>\n",
       "      <td>udah 10 tahun ajaa time flies.. eh tapi aku b...</td>\n",
       "      <td>10YearsOf5SOS udah 10 tahun ajaa time flies e...</td>\n",
       "      <td>udah 10 tahun ajaa time flies.. eh tapi aku b...</td>\n",
       "      <td>[10YearsOf5SOS, udah, 10, tahun, ajaa, time, f...</td>\n",
       "      <td>[udah, 10, tahun, ajaa, time, flies, .., eh, t...</td>\n",
       "      <td>Sentence: \"10YearsOf5SOS udah 10 tahun ajaa ti...</td>\n",
       "      <td>[10YearsOf5SOS_ADV, udah_VERB, 10_NUM, tahun_N...</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NANGIS BGT 2011 ENAK BANGET ALLAHUU NGINGETIN ...</td>\n",
       "      <td>NANGIS BGT 2011 ENAK BANGET ALLAHUU NGINGETIN ...</td>\n",
       "      <td>NANGIS BGT 2011 ENAK BANGET ALLAHUU NGINGETIN ...</td>\n",
       "      <td>NANGIS BGT 2011 ENAK BANGET ALLAHUU NGINGETIN ...</td>\n",
       "      <td>NANGIS BGT 2011 ENAK BANGET ALLAHUU NGINGETIN ...</td>\n",
       "      <td>NANGIS BGT 2011 ENAK BANGET ALLAHUU NGINGETIN ...</td>\n",
       "      <td>NANGIS BGT 2011 ENAK BANGET ALLAHUU NGINGETIN ...</td>\n",
       "      <td>[NANGIS, BGT, 2011, ENAK, BANGET, ALLAHUU, NGI...</td>\n",
       "      <td>[NANGIS, BGT, 2011, ENAK, BANGET, ALLAHUU, NGI...</td>\n",
       "      <td>Sentence: \"NANGIS BGT 2011 ENAK BANGET ALLAHUU...</td>\n",
       "      <td>[NANGIS_PROPN, BGT_PROPN, 2011_NUM, ENAK_PROPN...</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Liriknya itu looooü•∫üò≠\\n#10YearsOf5SOS https://t...</td>\n",
       "      <td>Liriknya itu looooü•∫üò≠\\n 10YearsOf5SOS</td>\n",
       "      <td>Liriknya itu looooü•∫üò≠\\n</td>\n",
       "      <td>Liriknya itu looooü•∫üò≠\\n 10YearsOf5SOS</td>\n",
       "      <td>Liriknya itu looooü•∫üò≠\\n</td>\n",
       "      <td>Liriknya itu loooo\\n 10YearsOf5SOS</td>\n",
       "      <td>Liriknya itu loooo\\n</td>\n",
       "      <td>[Liriknya, itu, loooo, 10YearsOf5SOS]</td>\n",
       "      <td>[Liriknya, itu, loooo]</td>\n",
       "      <td>Sentence: \"Liriknya itu loooo 10YearsOf5SOS\"  ...</td>\n",
       "      <td>[Liriknya_NOUN, itu_DET, loooo_VERB, 10YearsOf...</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>happy #10YearsOf5sos \\n\\n5sosfam itu fandom ke...</td>\n",
       "      <td>happy  10YearsOf5sos \\n\\n5sosfam itu fandom ke...</td>\n",
       "      <td>happy  \\n\\n5sosfam itu fandom kedua, 5sos yang...</td>\n",
       "      <td>happy  10YearsOf5sos \\n\\n5sosfam itu fandom ke...</td>\n",
       "      <td>happy  \\n\\n5sosfam itu fandom kedua, 5sos yang...</td>\n",
       "      <td>happy  10YearsOf5sos \\n\\n5sosfam itu fandom ke...</td>\n",
       "      <td>happy  \\n\\n5sosfam itu fandom kedua, 5sos yang...</td>\n",
       "      <td>[happy, 10YearsOf5sos, 5sosfam, itu, fandom, k...</td>\n",
       "      <td>[happy, 5sosfam, itu, fandom, kedua, ,, 5sos, ...</td>\n",
       "      <td>Sentence: \"happy 10YearsOf5sos 5sosfam itu fan...</td>\n",
       "      <td>[happy_NOUN, 10YearsOf5sos_NOUN, 5sosfam_NOUN,...</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            raw_text  \\\n",
       "0                      cepet bgt si‚Ä¶. #10YearsOf5SOS   \n",
       "2  #10YearsOf5SOS udah 10 tahun ajaa time flies.....   \n",
       "5  NANGIS BGT 2011 ENAK BANGET ALLAHUU NGINGETIN ...   \n",
       "7  Liriknya itu looooü•∫üò≠\\n#10YearsOf5SOS https://t...   \n",
       "8  happy #10YearsOf5sos \\n\\n5sosfam itu fandom ke...   \n",
       "\n",
       "                                   1st_clean_twitter  \\\n",
       "0                      cepet bgt si‚Ä¶.  10YearsOf5SOS   \n",
       "2   10YearsOf5SOS udah 10 tahun ajaa time flies.....   \n",
       "5  NANGIS BGT 2011 ENAK BANGET ALLAHUU NGINGETIN ...   \n",
       "7              Liriknya itu looooü•∫üò≠\\n 10YearsOf5SOS    \n",
       "8  happy  10YearsOf5sos \\n\\n5sosfam itu fandom ke...   \n",
       "\n",
       "                            1st_clean_twitter_gensim  \\\n",
       "0                                    cepet bgt si‚Ä¶.    \n",
       "2   udah 10 tahun ajaa time flies.. eh tapi aku b...   \n",
       "5  NANGIS BGT 2011 ENAK BANGET ALLAHUU NGINGETIN ...   \n",
       "7                            Liriknya itu looooü•∫üò≠\\n    \n",
       "8  happy  \\n\\n5sosfam itu fandom kedua, 5sos yang...   \n",
       "\n",
       "                                     2nd_punctuation  \\\n",
       "0                       cepet bgt si‚Ä¶  10YearsOf5SOS   \n",
       "2   10YearsOf5SOS udah 10 tahun ajaa time flies e...   \n",
       "5  NANGIS BGT 2011 ENAK BANGET ALLAHUU NGINGETIN ...   \n",
       "7              Liriknya itu looooü•∫üò≠\\n 10YearsOf5SOS    \n",
       "8  happy  10YearsOf5sos \\n\\n5sosfam itu fandom ke...   \n",
       "\n",
       "                              2nd_punctuation_gensim  \\\n",
       "0                                    cepet bgt si‚Ä¶.    \n",
       "2   udah 10 tahun ajaa time flies.. eh tapi aku b...   \n",
       "5  NANGIS BGT 2011 ENAK BANGET ALLAHUU NGINGETIN ...   \n",
       "7                            Liriknya itu looooü•∫üò≠\\n    \n",
       "8  happy  \\n\\n5sosfam itu fandom kedua, 5sos yang...   \n",
       "\n",
       "                                           3rd_emoji  \\\n",
       "0                       cepet bgt si‚Ä¶  10YearsOf5SOS   \n",
       "2   10YearsOf5SOS udah 10 tahun ajaa time flies e...   \n",
       "5  NANGIS BGT 2011 ENAK BANGET ALLAHUU NGINGETIN ...   \n",
       "7                Liriknya itu loooo\\n 10YearsOf5SOS    \n",
       "8  happy  10YearsOf5sos \\n\\n5sosfam itu fandom ke...   \n",
       "\n",
       "                                    3rd_emoji_gensim  \\\n",
       "0                                    cepet bgt si‚Ä¶.    \n",
       "2   udah 10 tahun ajaa time flies.. eh tapi aku b...   \n",
       "5  NANGIS BGT 2011 ENAK BANGET ALLAHUU NGINGETIN ...   \n",
       "7                              Liriknya itu loooo\\n    \n",
       "8  happy  \\n\\n5sosfam itu fandom kedua, 5sos yang...   \n",
       "\n",
       "                                       4th_tokenized  \\\n",
       "0                   [cepet, bgt, si‚Ä¶, 10YearsOf5SOS]   \n",
       "2  [10YearsOf5SOS, udah, 10, tahun, ajaa, time, f...   \n",
       "5  [NANGIS, BGT, 2011, ENAK, BANGET, ALLAHUU, NGI...   \n",
       "7              [Liriknya, itu, loooo, 10YearsOf5SOS]   \n",
       "8  [happy, 10YearsOf5sos, 5sosfam, itu, fandom, k...   \n",
       "\n",
       "                                4th_tokenized_gensim  \\\n",
       "0                               [cepet, bgt, si‚Ä¶, .]   \n",
       "2  [udah, 10, tahun, ajaa, time, flies, .., eh, t...   \n",
       "5  [NANGIS, BGT, 2011, ENAK, BANGET, ALLAHUU, NGI...   \n",
       "7                             [Liriknya, itu, loooo]   \n",
       "8  [happy, 5sosfam, itu, fandom, kedua, ,, 5sos, ...   \n",
       "\n",
       "                                            sentence  \\\n",
       "0  Sentence: \"cepet bgt si‚Ä¶ 10YearsOf5SOS\"   [‚àí T...   \n",
       "2  Sentence: \"10YearsOf5SOS udah 10 tahun ajaa ti...   \n",
       "5  Sentence: \"NANGIS BGT 2011 ENAK BANGET ALLAHUU...   \n",
       "7  Sentence: \"Liriknya itu loooo 10YearsOf5SOS\"  ...   \n",
       "8  Sentence: \"happy 10YearsOf5sos 5sosfam itu fan...   \n",
       "\n",
       "                                            5_tagged emotion  \n",
       "0  [cepet_PROPN, bgt_PROPN, si‚Ä¶_PROPN, 10YearsOf5...     sad  \n",
       "2  [10YearsOf5SOS_ADV, udah_VERB, 10_NUM, tahun_N...     sad  \n",
       "5  [NANGIS_PROPN, BGT_PROPN, 2011_NUM, ENAK_PROPN...   happy  \n",
       "7  [Liriknya_NOUN, itu_DET, loooo_VERB, 10YearsOf...     sad  \n",
       "8  [happy_NOUN, 10YearsOf5sos_NOUN, 5sosfam_NOUN,...   happy  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_model = pd.read_json('../dataset/tweet-cleaned.json', orient='index')\n",
    "rnn_model.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = rnn_model['sentence'].tolist()\n",
    "labels = rnn_model['emotion'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['sad', 'happy', 'angry', 'surprise', 'funny', 'hopeful', 'disgust',\n",
       "       'neutral', 'fear'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_model['emotion'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total length of the data is:         457\n",
      "No. of sad tagged sentences is:  23\n",
      "No. of happy tagged sentences is: 240\n",
      "No. of angry tagged sentences is:  28\n",
      "No. of surprise tagged sentences is: 10\n",
      "No. of funny tagged sentences is:  29\n",
      "No. of hopeful tagged sentences is: 29\n",
      "No. of disgust tagged sentences is:  42\n",
      "No. of neutral tagged sentences is: 40\n",
      "No. of fear tagged sentences is:  16\n"
     ]
    }
   ],
   "source": [
    "sad = rnn_model['emotion'][rnn_model.emotion == 'sad' ]\n",
    "happy = rnn_model['emotion'][rnn_model.emotion == 'happy' ]\n",
    "angry = rnn_model['emotion'][rnn_model.emotion == 'angry' ]\n",
    "surprise = rnn_model['emotion'][rnn_model.emotion == 'surprise' ]\n",
    "funny = rnn_model['emotion'][rnn_model.emotion == 'funny' ]\n",
    "hopeful = rnn_model['emotion'][rnn_model.emotion == 'hopeful' ]\n",
    "disgust = rnn_model['emotion'][rnn_model.emotion == 'disgust' ]\n",
    "neutral = rnn_model['emotion'][rnn_model.emotion == 'neutral' ]\n",
    "fear = rnn_model['emotion'][rnn_model.emotion == 'fear' ]\n",
    "\n",
    "\n",
    "print('Total length of the data is:         {}'.format(rnn_model.shape[0]))\n",
    "print('No. of sad tagged sentences is:  {}'.format(len(sad)))\n",
    "print('No. of happy tagged sentences is: {}'.format(len(happy)))\n",
    "print('No. of angry tagged sentences is:  {}'.format(len(angry)))\n",
    "print('No. of surprise tagged sentences is: {}'.format(len(surprise)))\n",
    "print('No. of funny tagged sentences is:  {}'.format(len(funny)))\n",
    "print('No. of hopeful tagged sentences is: {}'.format(len(hopeful)))\n",
    "print('No. of disgust tagged sentences is:  {}'.format(len(disgust)))\n",
    "print('No. of neutral tagged sentences is: {}'.format(len(neutral)))\n",
    "print('No. of fear tagged sentences is:  {}'.format(len(fear)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: \"10YearsOf5SOS udah 10 tahun ajaa time flies eh tapi aku baru dengerin lagu mereka pas 2014 sih seingetku\"   [‚àí Tokens: 18  ‚àí Token-Labels: \"10YearsOf5SOS <ADV> udah <VERB> 10 <NUM> tahun <NOUN> ajaa <PROPN> time <PROPN> flies <PROPN> eh <PROPN> tapi <CCONJ> aku <PRON> baru <ADJ> dengerin <NOUN> lagu <NOUN> mereka <PRON> pas <ADJ> 2014 <NUM> sih <PART> seingetku <ADV>\"]\n",
      "sad\n"
     ]
    }
   ],
   "source": [
    "print(sentences[1])\n",
    "print(labels[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<propn>', 2842),\n",
       " ('<noun>', 1418),\n",
       " ('<verb>', 650),\n",
       " ('sentence:', 457),\n",
       " ('tokens:', 457),\n",
       " ('token-labels:', 457),\n",
       " ('<adv>', 412),\n",
       " ('<punct>\"]', 387),\n",
       " ('<pron>', 337),\n",
       " ('<adp>', 264),\n",
       " ('<adj>', 221),\n",
       " ('<part>', 151),\n",
       " ('jeno', 146),\n",
       " ('dan', 142),\n",
       " ('<num>', 132),\n",
       " ('ini', 132),\n",
       " ('<det>', 115),\n",
       " ('sama', 104),\n",
       " ('brightwinxaaa2021', 104),\n",
       " ('8yearswithjeno', 103)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "all_words = []\n",
    "for line in list(rnn_model['sentence']):\n",
    "    words = line.split()\n",
    "    for word in words:\n",
    "      if(len(word)>2):\n",
    "        all_words.append(word.lower())\n",
    "    \n",
    "    \n",
    "Counter(all_words).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_size = int(len(sentences) * 0.8)\n",
    "\n",
    "training_sentences = sentences[:training_size]\n",
    "training_labels = labels[:training_size]\n",
    "testing_sentences = sentences[training_size:]\n",
    "testing_labels = labels[training_size:]\n",
    "\n",
    "# put labels into list to use later:\n",
    "\n",
    "training_labels_final = np.array(training_labels)\n",
    "testing_labels_final = np.array(testing_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing And Pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 1000\n",
    "embedding_dim = 16\n",
    "max_length = 280\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<OOV>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(training_sentences)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "training_sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "training_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
    "testing_padded = pad_sequences(testing_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data size: 365\n",
      "Test Data size 92\n",
      "Vocab size: 1000\n",
      "Max length: 280\n",
      "tp: 365\n",
      "tp: 92\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Data size:\", len(training_sequences))\n",
    "print(\"Test Data size\", len(testing_sequences))\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "print(\"Max length:\", max_length)\n",
    "\n",
    "print('tp:',len(training_padded))\n",
    "print('tp:',len(testing_padded))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_Evaluate(model):\n",
    "    #accuracy of model on training data\n",
    "    acc_train=model.score(X_train, y_train)\n",
    "    #accuracy of model on test data\n",
    "    acc_test=model.score(X_test, y_test)\n",
    "    \n",
    "    print('Accuracy of model on training data : {}'.format(acc_train*100))\n",
    "    print('Accuracy of model on testing data : {} \\n'.format(acc_test*100))\n",
    "\n",
    "    # Predict values for Test dataset\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Print the evaluation metrics for the dataset.\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Compute and plot the Confusion matrix\n",
    "    cf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    categories  = ['sad', 'happy', 'angry', 'surprise', 'funny', 'hopeful', 'disgust', 'neutral', 'fear']\n",
    "    group_names = ['True Neg','False Pos', 'False Neg','True Pos']\n",
    "    group_percentages = ['{0:.2%}'.format(value) for value in cf_matrix.flatten() / np.sum(cf_matrix)]\n",
    "\n",
    "    labels = [f'{v1}\\n{v2}' for v1, v2 in zip(group_names,group_percentages)]\n",
    "    labels = np.asarray(labels).reshape(2,2)\n",
    "\n",
    "    # sns.heatmap(cf_matrix, annot = labels, cmap = 'Reds',fmt = '',\n",
    "    #             xticklabels = categories, yticklabels = categories)\n",
    "\n",
    "    # plt.xlabel(\"Predicted values\", fontdict = {'size':14}, labelpad = 10)\n",
    "    # plt.ylabel(\"Actual values\"   , fontdict = {'size':14}, labelpad = 10)\n",
    "    # plt.title (\"Confusion Matrix\", fontdict = {'size':18}, pad = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10072/2575452390.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mlg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mhistory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mmodel_Evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "lg = LogisticRegression()\n",
    "history=lg.fit(X_train, y_train)\n",
    "model_Evaluate(lg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear svc\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model on training data : 41.64383561643836\n",
      "Accuracy of model on testing data : 18.478260869565215 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.00      0.00      0.00         6\n",
      "     disgust       0.00      0.00      0.00         5\n",
      "        fear       0.20      0.50      0.29         4\n",
      "       funny       0.11      0.12      0.12         8\n",
      "       happy       0.50      0.25      0.33        52\n",
      "     hopeful       0.00      0.00      0.00         6\n",
      "     neutral       0.07      0.17      0.10         6\n",
      "         sad       0.00      0.00      0.00         3\n",
      "    surprise       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.18        92\n",
      "   macro avg       0.10      0.12      0.09        92\n",
      "weighted avg       0.31      0.18      0.22        92\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aryo\\Documents\\spadadikti-ta\\ta_nlp\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "svm = LinearSVC()\n",
    "svm.fit(X_train, y_train)\n",
    "model_Evaluate(svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random forest\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model on training data : 99.45205479452055\n",
      "Accuracy of model on testing data : 53.2608695652174 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.00      0.00      0.00         6\n",
      "     disgust       0.00      0.00      0.00         5\n",
      "        fear       0.00      0.00      0.00         4\n",
      "       funny       0.00      0.00      0.00         8\n",
      "       happy       0.62      0.90      0.73        52\n",
      "     hopeful       1.00      0.17      0.29         6\n",
      "     neutral       0.25      0.17      0.20         6\n",
      "         sad       0.00      0.00      0.00         3\n",
      "    surprise       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.53        92\n",
      "   macro avg       0.21      0.14      0.14        92\n",
      "weighted avg       0.43      0.53      0.45        92\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aryo\\Documents\\spadadikti-ta\\ta_nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Aryo\\Documents\\spadadikti-ta\\ta_nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Aryo\\Documents\\spadadikti-ta\\ta_nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators = 20, criterion = 'entropy', max_depth=50)\n",
    "rf.fit(X_train, y_train)\n",
    "model_Evaluate(rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bernoulli NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bernoulli Naive Bayes\n",
    "from sklearn.naive_bayes import BernoulliNB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model on training data : 13.150684931506849\n",
      "Accuracy of model on testing data : 8.695652173913043 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.00      0.00      0.00         6\n",
      "     disgust       0.00      0.00      0.00         5\n",
      "        fear       0.00      0.00      0.00         4\n",
      "       funny       0.12      0.25      0.17         8\n",
      "       happy       0.42      0.10      0.16        52\n",
      "     hopeful       0.00      0.00      0.00         6\n",
      "     neutral       0.06      0.17      0.09         6\n",
      "         sad       0.00      0.00      0.00         3\n",
      "    surprise       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.09        92\n",
      "   macro avg       0.07      0.06      0.05        92\n",
      "weighted avg       0.25      0.09      0.11        92\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aryo\\Documents\\spadadikti-ta\\ta_nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Aryo\\Documents\\spadadikti-ta\\ta_nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Aryo\\Documents\\spadadikti-ta\\ta_nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "nb = BernoulliNB()\n",
    "nb.fit(X_train, y_train)\n",
    "model_Evaluate(nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`What is RNN?`\n",
    "\n",
    "Recurrent neural networks (RNN) are the state of the art algorithm for sequential data and are used by Apple's Siri and and Google's voice search. It is the first algorithm that remembers its input, due to an internal memory, which makes it perfectly suited for machine learning problems that involve sequential data\n",
    "\n",
    "`Embedding Layer`\n",
    "\n",
    "Embedding layer is one of the available layers in Keras. This is mainly used in Natural Language Processing related applications such as language modeling, but it can also be used with other tasks that involve neural networks. While dealing with NLP problems, we can use pre-trained word embeddings such as GloVe. Alternatively we can also train our own embeddings using Keras embedding layer.\n",
    "\n",
    "`LSTM layer`\n",
    "\n",
    "Long Short Term Memory networks, usually called ‚ÄúLSTMs‚Äù , were introduced by Hochreiter and Schmiduber. These have widely been used for speech recognition, language modeling, sentiment analysis and text prediction. Before going deep into LSTM, we should first understand the need of LSTM which can be explained by the drawback of practical use of Recurrent Neural Network (RNN). So, lets start with RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "model.add(tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length))\n",
    "model.add(tf.keras.layers.Bidirectional(\n",
    "    tf.keras.layers.LSTM(embedding_dim,\n",
    "                         return_sequences=True)\n",
    "))\n",
    "model.add(tf.keras.layers.Dense(6, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.Adam(0.01),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'checkpoint2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10072/828488050.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m )\n\u001b[0;32m      5\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_padded\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_labels_final\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtesting_padded\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtesting_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcheckpoint2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'checkpoint2' is not defined"
     ]
    }
   ],
   "source": [
    "callbacks = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto',\n",
    "    baseline=None, restore_best_weights=False\n",
    ")\n",
    "num_epochs=10\n",
    "history = model.fit(training_padded, training_labels_final, epochs=50,validation_data=(testing_padded, testing_labels),callbacks=[checkpoint2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Sentence: \"cepet bgt si‚Ä¶ 10YearsOf5SOS\"   [‚àí T...\n",
       "2       Sentence: \"10YearsOf5SOS udah 10 tahun ajaa ti...\n",
       "5       Sentence: \"NANGIS BGT 2011 ENAK BANGET ALLAHUU...\n",
       "7       Sentence: \"Liriknya itu loooo 10YearsOf5SOS\"  ...\n",
       "8       Sentence: \"happy 10YearsOf5sos 5sosfam itu fan...\n",
       "                              ...                        \n",
       "3744    Sentence: \"maaf pakbu kameramen sblmnya saya m...\n",
       "3752    Sentence: \"JUNGKOOK LOVE U LO NUTUPIN ABS PAS ...\n",
       "3771    Sentence: \"yerimwiyese IYAA PDHL MSI DIPANGGUN...\n",
       "3772    Sentence: \"BST x FAKE LOVE emg terbaik\"   [‚àí T...\n",
       "3836    Sentence: \"BadmintonTalk Semifinal menanti sem...\n",
       "Name: sentence, Length: 457, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_model['sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0 ...    1   50    5]\n",
      " [   0    0    0 ...   16  885   10]\n",
      " [   0    0    0 ...    1   50    5]\n",
      " ...\n",
      " [   0    0    0 ...    1   52    1]\n",
      " [   0    0    0 ...    2  710   13]\n",
      " [   0    0    0 ...    2 2387    5]]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import regularizers\n",
    "\n",
    "max_words = 5000\n",
    "max_len = 200\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(rnn_model.sentence)\n",
    "sequences = tokenizer.texts_to_sequences(rnn_model.sentence)\n",
    "tweets = pad_sequences(sequences, maxlen=max_len)\n",
    "print(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tweets, rnn_model.emotion.values, test_size=0.2, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train (365, 200)\n",
      "y_train (365,)\n",
      "\n",
      "X_test (92, 200)\n",
      "y_test (92,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train\", X_train.shape)\n",
    "print(\"y_train\", y_train.shape)\n",
    "print()\n",
    "print(\"X_test\", X_test.shape)\n",
    "print(\"y_test\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "SAVE_PERIOD = 10\n",
    "BATCH_SIZE = 20\n",
    "STEPS_PER_EPOCH = y_train.size / BATCH_SIZE\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(layers.Embedding(max_words, 128))\n",
    "model2.add(layers.LSTM(64,dropout=0.5))\n",
    "model2.add(layers.Dense(16, activation='relu'))\n",
    "model2.add(layers.Dense(8, activation='relu'))\n",
    "model2.add(layers.Dense(1,activation='sigmoid'))\n",
    "model2.compile(optimizer='adam',loss='binary_crossentropy', metrics=['accuracy'])\n",
    "checkpoint2 = ModelCheckpoint(\n",
    "    filepath=\"rnn_model.hdf5\",\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "history = model2.fit(X_train, y_train, epochs=50,validation_data=(X_test, y_test),callbacks=[checkpoint2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "raw_text                    object\n",
       "1st_clean_twitter           object\n",
       "1st_clean_twitter_gensim    object\n",
       "2nd_punctuation             object\n",
       "2nd_punctuation_gensim      object\n",
       "3rd_emoji                   object\n",
       "3rd_emoji_gensim            object\n",
       "4th_tokenized               object\n",
       "4th_tokenized_gensim        object\n",
       "sentence                    object\n",
       "5_tagged                    object\n",
       "emotion                     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_model.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv1D, Bidirectional, LSTM, Dense, Input, Dropout\n",
    "from tensorflow.keras.layers import SpatialDropout1D\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300\n",
    "LR = 1e-3\n",
    "BATCH_SIZE = 1024\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE = 0.8\n",
    "MAX_NB_WORDS = 100000\n",
    "MAX_SEQUENCE_LENGTH = 30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embeddings_index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16060/2864424602.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0membedding_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEMBEDDING_DIM\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mword_index\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m   \u001b[0membedding_vector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0membeddings_index\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0membedding_vector\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0membedding_matrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0membedding_vector\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'embeddings_index' is not defined"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "  embedding_vector = embeddings_index.get(word)\n",
    "  if embedding_vector is not None:\n",
    "    embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embedding_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16060/1316724230.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m embedding_layer = tf.keras.layers.Embedding(vocab_size,\n\u001b[0;32m      2\u001b[0m                                           \u001b[0mEMBEDDING_DIM\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m                                           \u001b[0mweights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0membedding_matrix\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m                                           \u001b[0minput_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mMAX_SEQUENCE_LENGTH\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                                           trainable=False)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'embedding_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "embedding_layer = tf.keras.layers.Embedding(vocab_size,\n",
    "                                          EMBEDDING_DIM,\n",
    "                                          weights=[embedding_matrix],\n",
    "                                          input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                          trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedding_sequences = embedding_layer(sequence_input)\n",
    "x = SpatialDropout1D(0.2)(embedding_sequences)\n",
    "x = Conv1D(64, 5, activation='relu')(x)\n",
    "x = Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2))(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "outputs = Dense(1, activation='sigmoid')(x)\n",
    "model = tf.keras.Model(sequence_input, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c4456429143f5684bcabf0b0e6c3060130a47553bf4e1061849a12f626e0dc8e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('ta_nlp': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
